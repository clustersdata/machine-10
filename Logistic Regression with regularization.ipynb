{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padraic Flood, 13402438, 4BMS\n",
    "# Description of the algorithm and design decisions\n",
    "## The data\n",
    "\n",
    "Given training data consisting of a vector, $\\textbf{y}$, with elements $y^{(i)} \\in \\{0,1\\}, i = 1, .. , N$ and a feature matrix $\\textbf{X}$ with rows $(\\textbf{x}^{i})^T$ , where the first column of $\\textbf{X}$ is the **all ones column representing the intercept**. So $\\textbf{X}$ has dimensions $N$ x $(M+1)$, where $M$ is number of features. $\\textbf{X}$ must have **numerical entries**.\n",
    "## The model - Logistic Regression\n",
    "In logistic regression, with a coefficient vector $\\theta$, we model the data using a **sigmoid function**, $h_\\theta(x)$, as follows for an input $\\textbf{x}$ and output $y$:\n",
    "\n",
    "$$ P(y = 1 | \\textbf{x}; \\theta) = h_\\theta(\\textbf{x}) = \\frac{1}{1 + exp(-\\theta^T\\textbf{x})} $$\n",
    "and $$ P(y = 0 | \\textbf{x}; \\theta) = 1 - h_\\theta(\\textbf{x}) $$\n",
    "combining these gives:\n",
    "$$ P(y|\\textbf{x};\\theta) = h_\\theta(\\textbf{x})^{y}(1 - h_\\theta(\\textbf{x}))^{1-y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of coefficients $\\theta$\n",
    "Assuming the data is **independent** we can calculate the likelihood of the data as follows:\n",
    "$$L(\\theta) = P(\\textbf{y}|\\textbf{X};\\theta) = \\prod_{i=1}^N P(y^{i}|\\textbf{x}^i;\\theta) = \\prod_{i=1}^N h_\\theta(\\textbf{x}^{i})^{y^{i}}(1 - h_\\theta(\\textbf{x}^{i}))^{1-y^{i}} $$\n",
    "Taking the log of this function makes it easier to differentiate and maximising the likelihood is the same as maximising the log likelihood, $l(\\theta)$, since the log function is **monotonic increasing**\n",
    "$$ l(\\theta) = \\log(L(\\theta)) = \\sum_{i=1}^N  [{y^{i}}\\log h_\\theta(\\textbf{x}^{i}) + (1-y^{i})\\log(1 - h_\\theta(\\textbf{x}^{i}))]$$\n",
    "$$ l(\\theta) = \\sum_{i=1}^N  [{y^{i}}\\log( \\frac{h_\\theta(\\textbf{x}^{i})}{1-h_\\theta(\\textbf{x}^{i})}) + \\log(\\frac{1}{1 + exp(\\theta^T\\textbf{x})})]$$\n",
    "$$ l(\\theta) = \\sum_{i=1}^N  [y^{i}\\theta^T\\textbf{x}^{i} - \\log({1 + exp(\\theta^T\\textbf{x}^{i})})]$$\n",
    "taking the partial derivatives for $j = 0,1, .., M$, where M is the number of features of $\\textbf{X}$:\n",
    "$$ \\frac{\\partial l(\\theta)}{\\partial \\theta_j} = \\sum_{i=1}^N  [y^{i}\\textbf{x}_j^{i} - \\frac{1}{1 + exp(\\theta^T\\textbf{x}^{i})} exp(\\theta^T\\textbf{x}^{i}) \\textbf{x}_j^{i}] = \\sum_{i=1}^N  [y^{i}\\textbf{x}_j^{i} -  h_\\theta(\\textbf{x}^{i})\\textbf{x}_j^{i}] = \\sum_{i=1}^N  [y^{i} -  h_\\theta(\\textbf{x}^{i})]\\textbf{x}^{i}_j$$\n",
    "Or in vector-matrix product form:\n",
    "\n",
    "let $\\textbf{h}$ be the column vector with $\\textbf{h}_i = h_\\theta(\\textbf{x}^i)$ for $i = 1, .. , N$\n",
    "\n",
    "then the above partial derivatives, $\\frac{\\partial l(\\theta)}{\\partial \\theta_j}$, can be rewritten as:\n",
    "$$ \\nabla l(\\theta) = (\\textbf{y} - \\textbf{h})^T \\textbf{X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "To reduce overfitting I implemented regularisation of the coefficients $\\theta$. I want to **maximise the log likelihood** of the training data but also **minimise the complexity** of the coefficients. Note: maximising the log likelihood is the same as minimising the negative of the log likelihood. So I want to minimise the following **cost function**:\n",
    "$$ J(\\theta) = -l(\\theta) + \\lambda \\sum_{i=1}^M |\\theta_i^q| $$\n",
    "I have implemented q = 2 (penalty='l2'). But $q = 1$ (penalty='l1') gives a non-differentiable cost function which is harder to implement. lambda is a python keywords so I have decided instead use the scikit convention of using the parameter C for complexity and then setting $\\lambda$ as $\\lambda = \\frac{1}{C}$. So a higher C means a higher model complexity (lower regularisation).\n",
    "\n",
    "Note: conventionally this is divided by the number of samples but here I have chosen not to. The two methods are equivalent under a scaling of parameters (learning rate and lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "It can be shown that this is a convex function of $\\theta$ and so we can minimise it using gradient descent. For q = 2:\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = - \\frac{\\partial l(\\theta)}{\\partial \\theta_j} + \\frac{2}{C}\\theta_j$$\n",
    "for $j=1,..,M$ and for the intercept (which we don't regularise):\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = - \\frac{\\partial l(\\theta)}{\\partial \\theta_0}$$\n",
    "Then gradient descent repeats:\n",
    "$$ \\theta^{t+1} = \\theta^{t} - \\alpha \\nabla J(\\theta^t)$$\n",
    "where $\\alpha$ is the learning rate and $\\nabla J(\\theta)$ means the gradient of the cost function (i.e the vector of partial derivatives).\n",
    "This repeats until convergence measured either by a max number of iterations or by the change in the cost function being under a certain tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method.\n",
    "In this case the cost function is convex and so finding the the global minimum is equivalent to finding a local minimum. This can be done by finding a point with zero gradient. Then convexity ensures it is a minimum and not a maximum. Finding a **root of the gradient function** can be done with newton's method, which in the multidimensional case is given by:\n",
    "$$ \\theta^{t+1} = \\theta^{t} - H^{-1} \\nabla J(\\theta^t) $$\n",
    "where H is the hessian matrix of $J(\\theta)$\n",
    "$$H_{ij} = \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_i \\partial \\theta_j} $$\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial \\theta_j} = \\sum_{k=1}^N  y^k\\textbf{x}_j^k - \\sum_{k=1}^N h_\\theta(\\textbf{x}^k)\\textbf{x}_j^k$$\n",
    "$$ \\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_j} = 0 -  \\sum_{k=1}^N \\frac{\\partial }{\\partial \\theta_i} (h_\\theta(\\textbf{x}^k))\\textbf{x}_j^k$$\n",
    "$$ \\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_j} = -  \\sum_{k=1}^N  (1-h_\\theta(\\textbf{x}^k))(h_\\theta(\\textbf{x}^k))\\textbf{x}_i^k\\textbf{x}_j^k$$\n",
    "adding regularisation:\n",
    "$$ H(J) =  - H(l) + \\frac{2}{C}I_{M+1}$$\n",
    "where I in the identity matrix, $H(l)$ is the hessian of $l(\\theta)$, $H(J)$ is the hessian of $J(\\theta)$ used in newton method's update equation above.\n",
    "\n",
    "Newton's method takes far fewer iterations to converge than Gradient Descent. Its derivation using a Taylor expansion can be used to show it has **asymptotic quadratic convergence**. Also, due to the convexity of the cost function, it is guaranteed to converge. However, it requires inverting the Hessian matrix which can be an expensive operation if there is a very large number of features. There are heuristics to avoid this problem such as only inverting the hessian on a few iterations (maybe only once) and then trying to approximate the new inverse on subsequent iterations. Or you could try to approximate H by a diagonal matrix. Then inverting a diagonal matrix requires only taking the reciprocal of the diagonal entries. However I have not implemented these methods here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic regression\n",
    "Often we are modelling more than two classes. To model this using logistic regression I implemented one-vs-rest logistic regression. Given $k$ target classes I model this as $k$ separate binary classification problem. So the $j$th class is set to 1 and all other classes are set to 0, for $j = 1,..,k$. Fitting a binary logistic model on this gives $\\theta_j$. For a new test case, $\\textbf{x}_{test}$, I calculate the probabilities $P(y_{test} = j | \\textbf{x}_{test}; \\theta_j) = h_{\\theta_j}(x_{test})$ for $j = 1,..,k$. If all misclassification costs are equal we can then predict $\\hat{y} = \\underset{j = 1,..,k}{\\operatorname{argmax}} h_{\\theta_j}(x_{test})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "I used a similar basic API to Scikit to make it familiar to a new user and to allow it to work with other scikit functions and pipelines. This includes the same parameter names (tol, max\\_iter, C, penalty..) as well as some variable names (coef\\_, intercept\\_, classes\\_) and the scikit [convention](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/underscore-convention.md) of using an underscore at the end of a variable name to indicate that it is estimated from the data. Note: I have NOT looked at the actual source code of scikit estimators, I am just attempting to keep it consistent with their public APIs.\n",
    "* The data can be read in with the function **read_data**\n",
    "* I implemented the function train_test_split to split the data\n",
    "* I implement feature scaling with the function **standardise**. This should speed up convergence for gradient descent. Although I did not notice any effect on the small dataset I tested it on.\n",
    "* The model parameters are passed to the constructor of LogisticRegression. The model is fit to training data with the **fit** method. Predictions on test data are found with the **predict** method. And probability estimates are gotten with the **predict_proba** method.\n",
    "* My algorithm only supports numerical attributes. Although categorical attributes could be first converted to numerical representations using the tools provided in scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results\n",
    "* Both newton's method and gradient descent got average **accuracies above 95%** on the test sets of \"owls.csv\" with 10 random splits. I set the parameter C manually which might lead to a slight bias if I tried too many C values. A validation set could be used to avoid this.\n",
    "* They achieved similarly high accuracies on the iris dataset.\n",
    "* Full detailed results are **saved to the file \"output\"** and at the bottom of this PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and conclusions\n",
    "* **Newton's method** took **far fewer iterations** than gradient descent. Also since the Hessian matrix was only a 4x4 matrix in this case inverting it was not computationally intense and so newton's method was **much faster** than gradient descent. For a dataset with thousands of features it might be more efficient to use stochastic gradient descent.\n",
    "* The lack of need to set the learning rate for newton's method makes it more convenient \n",
    "* With newton's method the coefficients sometimes started to explode in size. I think this is caused by perfectly separable training data which causes the optimal fit to approach a step function. This leads to a singular (not invertible) Hessian matrix. I avoided this with **L2 regularisation** and by setting a smaller max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* andrew ng's stanford machine learning course: https://www.youtube.com/watch?v=nLKOQfKLUks for newton's method and\n",
    "https://www.youtube.com/watch?v=HZ4cvaztQEs for gradient descent for some details. But all the implementation and documentation here is entirely my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, max_iter=10000, learning_rate=.01, penalty='l2',\n",
    "                 tol=1e-4, C=1.0, fit_method='gradient_descent'):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.C = C\n",
    "        self.penalty = penalty\n",
    "        self.tol = tol\n",
    "        self.fit_method = fit_method\n",
    "        \n",
    "    def _call_fit_algorithm(self, X, y, index=0):\n",
    "        \"\"\"Fit coefficients with gradient descent or newton's method\n",
    "        index: the class being fit in multiclass case (0 for binary case)\n",
    "        \"\"\"\n",
    "        if self.fit_method == 'gradient_descent':\n",
    "            self._gradient_descent(X, y, index)\n",
    "        elif self.fit_method == 'newton':\n",
    "            self._newtons_method(X, y, index)\n",
    "        else:\n",
    "            raise ValueError('error incorrect fit_method')\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the coefficents using training data X, y. \n",
    "        X: 2D numpy array with numerical attributes as columns \n",
    "           and samples as rows\n",
    "        y: numpy array of labels\"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.multi_class = self.num_classes_ > 2\n",
    "        # Add a column of ones to X for the intercept:\n",
    "        X = np.c_[np.ones(len(X)),X]\n",
    "        \n",
    "        # Multi-class case:\n",
    "        if self.multi_class:\n",
    "            # Initialise coefficients to zero\n",
    "            self.theta_ = np.zeros((self.num_classes_, X.shape[1]))\n",
    "            # Track the number of iterations taken for each class\n",
    "            self.n_iter = np.zeros(self.num_classes_, dtype=np.uint32)\n",
    "            # Fit a logistic regression model for each class\n",
    "            for i, c in enumerate(self.classes_):\n",
    "                # 1's for this class and 0's for all others\n",
    "                y_c = 1*(y == c)\n",
    "                self._call_fit_algorithm(X, y_c, i)\n",
    "        \n",
    "        # Binary case:\n",
    "        else:\n",
    "            # Default second class to positive label\n",
    "            y_01 = 1*(y == self.classes_[1])\n",
    "            # Make the coefficients 2D array to be consistent with multi-class.\n",
    "            self.theta_ = np.zeros((1, X.shape[1]))\n",
    "            # An array to be consistent with multi_class case. \n",
    "            # Make it an integer to improve asthetics of output.\n",
    "            self.n_iter = np.zeros(1, dtype=np.uint32)\n",
    "            self._call_fit_algorithm(X, y_01)\n",
    "        \n",
    "        # Set parameters names to those used by scikit \n",
    "        # to make it familiar to new user.\n",
    "        self.intercept_ = self.theta_[:,0]\n",
    "        self.coef_ = self.theta_[:,1:]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return numpy array of predicted classes for test set X\"\"\"\n",
    "        prob = self.predict_proba(X)\n",
    "        # Find the index of the class with the maximum probability estimate \n",
    "        # for each test sample\n",
    "        most_probable_indices = np.argmax(prob, axis = 1)\n",
    "        # Convert from indices back to original class names\n",
    "        most_probable_classes = self.classes_[most_probable_indices]\n",
    "        return most_probable_classes\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return 2D numpy array of probabilities for test set X\n",
    "        The classes are given by the columns \n",
    "        and the test samples are given by the rows.\n",
    "        \"\"\"\n",
    "        # Add a column of ones to X for the intercept:\n",
    "        X = np.c_[np.ones(len(X)),X]\n",
    "        try:\n",
    "            prob = self._h(0, X)\n",
    "            if self.multi_class:\n",
    "                for i in range(1, self.num_classes_):\n",
    "                    prob = np.c_[prob, self._h(i, X)]\n",
    "            else:\n",
    "                # Binary case\n",
    "                # Note: I chose to model the second class as the positive label.\n",
    "                prob = np.c_[1 - prob, prob]\n",
    "            return prob\n",
    "        except AttributeError:\n",
    "            print(\"Error: The model must first be fit using the fit method\")\n",
    "        \n",
    "    def _newtons_method(self, X, y, index):\n",
    "        \"\"\"Fits binary logistic model for class given by index. \n",
    "        Stores the coefficients in self.theta_[index].\n",
    "        y must be in 0,1 form\n",
    "        index: the class being fit in multiclass case (0 for binary case)\n",
    "        \"\"\"\n",
    "        while self.n_iter[index] < self.max_iter:\n",
    "            old_cost = self._cost(X, y, index)\n",
    "            h = self._h(index, X)\n",
    "            # Calculate the Hessian matrix\n",
    "            H = np.zeros((X.shape[1],X.shape[1]))\n",
    "            for k, x_k in enumerate(X):\n",
    "                H += (1-h[k])*h[k]*np.outer(x_k, x_k)\n",
    "            if self.penalty == 'l2':\n",
    "                H += (2.0/self.C) * np.identity(X.shape[1])\n",
    "            # Invert the Hessian\n",
    "            try:\n",
    "                H_inv = np.linalg.inv(H)\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Sometimes the coefficients explode to infinity \n",
    "                # due to perfect seperation of the data.\n",
    "                # This can be handled by more aggressive L2 regularization \n",
    "                # or by setting a lower max_iter.\n",
    "                raise RuntimeError('Convergence error')\n",
    "            # Apply newton's method to update coefficients\n",
    "            self.theta_[index] -= H_inv.dot(self._grad_cost(X,y,index))\n",
    "            \n",
    "            # STOP if converged\n",
    "            new_cost = self._cost(X, y, index)\n",
    "            if abs(new_cost - old_cost) < self.tol:\n",
    "                break\n",
    "            self.n_iter[index] += 1\n",
    "\n",
    "    def _gradient_descent(self, X, y, index):\n",
    "        \"\"\"Fits binary logistic model for class given by index. \n",
    "        Stores the coefficients in self.theta_.\n",
    "        y must be in 0,1 form\n",
    "        index: the class being fit in multiclass case (0 for binary case)\n",
    "        \"\"\"\n",
    "        while self.n_iter[index] < self.max_iter:\n",
    "            old_cost = self._cost(X, y, index)\n",
    "            self.theta_[index] -= self.learning_rate *\\\n",
    "                                    self._grad_cost(X, y, index)\n",
    "            new_cost = self._cost(X, y, index)\n",
    "            # STOP if convergence is met.\n",
    "            if abs(new_cost - old_cost) < self.tol:\n",
    "                break\n",
    "            self.n_iter += 1\n",
    "            \n",
    "    def _h(self, index, X):\n",
    "        \"\"\"Hypothesis function for current coefficients values.\n",
    "        Returns numpy array of probability of the class, given by index, \n",
    "        for each row of X\n",
    "        index: the class being fit in multiclass case (0 for binary case)\"\"\"\n",
    "        return 1.0/(1 + np.exp(-X.dot(self.theta_[index])))\n",
    "    \n",
    "    def _cost(self, X, y, index):\n",
    "        \"\"\"Return the cost function J(theta) = -l(theta) + (regularization)\n",
    "        y must be in 0,1 form\n",
    "        index: the class being fit in multiclass case (0 for binary case)\n",
    "        \"\"\"\n",
    "        h = self._h(index, X)\n",
    "        return -(np.sum(y.dot(np.log(h)) + (1-y).dot(np.log(1-h)))) +\\\n",
    "                self._complexity(index)\n",
    "    \n",
    "    def _grad_cost(self, X, y, index):\n",
    "        \"\"\"Return the gradient of the cost function\"\"\"\n",
    "        error = -(y - self._h(index, X))\n",
    "        if self.penalty == 'l2':\n",
    "            # Do not regularize the intercept\n",
    "            return error.dot(X) + (1.0/self.C) * 2 *\\\n",
    "                    np.insert((1.0/self.C) * 2 * self.theta_[index,1:],0,0)\n",
    "        # L1 regularization\n",
    "        #elif self.penalty == 'l1':\n",
    "        #    print 'l1 penalty not yet implemented'\n",
    "        # No regularization\n",
    "        elif self.penalty is None:\n",
    "            return error.dot(X)\n",
    "        else:\n",
    "            print \"Error: penalty must be either 'l1' or 'l2' or None\"\n",
    "            \n",
    "    def _complexity(self, index):\n",
    "        \"\"\"Return the measure of the complexity of the model \n",
    "        under different regularization conditions.\n",
    "        index: the class being fit in multiclass case (0 for binary case)\"\"\"\n",
    "        if self.penalty == 'l2':\n",
    "            return (1.0/self.C) * np.sum(np.square(self.theta_[index,1:])) \n",
    "        elif self.penalty == 'l1':\n",
    "            return (1.0/self.C) * np.sum(np.abs(self.theta_[index,1:])) \n",
    "        elif self.penalty is None:\n",
    "            return 0\n",
    "        else:\n",
    "            raise ValueError(\"Error: penalty must be either \\\n",
    "                                'l1' or 'l2' or None\")\n",
    "\n",
    "\n",
    "def read_data(filename, target, contains_header=True, attribute_names=None):\n",
    "    \"\"\"File must be a csv.\n",
    "    contains_header: True if header is in file as first row.\n",
    "    target: The name of the column representing the target \n",
    "            (output, dependent variable) of the model.\n",
    "    Returns tuple of (X, y) where X is the feature matrix and y is labels.\"\"\"\n",
    "    if contains_header == False:\n",
    "        d = pd.read_csv(filename, header=None, names=attribute_names)\n",
    "    else:\n",
    "        d = pd.read_csv(filename)\n",
    "    y = d[target]\n",
    "    X = d.drop(target, axis=1)\n",
    "    # convert from pandas dataframe to numpy array\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    return (X, y)\n",
    "\n",
    "def standardise(X):\n",
    "    \"\"\"standardise columns of X to give mean 0, variance 1.\"\"\"\n",
    "    return(X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "def train_test_split(X, y, test_size):\n",
    "    \"\"\"test_size: fraction to put in test set\"\"\"\n",
    "    p = np.random.permutation(len(y))\n",
    "    split_index = int(round(len(y)*test_size))\n",
    "    test = p[:split_index]\n",
    "    train = p[split_index:]\n",
    "    return (X[train], X[test], y[train], y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracies [ 0.97  0.94  1.    0.96  0.98  0.97  0.99  0.98  0.96  0.96]\n",
      "test accuracies [ 0.96  1.    0.93  0.98  0.91  0.93  0.91  0.93  0.98  1.  ]\n",
      "mean training accuracy 0.97, standard deviation 0.02\n",
      "mean test accuracy 0.95, standard deviation 0.03\n"
     ]
    }
   ],
   "source": [
    "# INPUT file here\n",
    "X, y = read_data('owls15.csv', contains_header=False, \n",
    "                 attribute_names = ['body-length', 'wing-length', 'body-width',\n",
    "                                    'wing-width', 'type'],\n",
    "                 target = 'type')\n",
    "X = standardise(X)\n",
    "\n",
    "test_scores = np.array([])\n",
    "train_scores = np.array([])\n",
    "out = open('output','w')\n",
    "for i in range(10):\n",
    "    out.write('Split ' + str(i+1) + '\\n')\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size = .33)\n",
    "    logit = LogisticRegression(C=10, tol=1e-2, penalty='l2',\n",
    "                               fit_method='newton', max_iter=100)\n",
    "    #logit = LogisticRegression(C=15, tol=1e-3, penalty=None, max_iter=50000,\n",
    "    #                          fit_method='gradient_descent', learning_rate=.05)\n",
    "    logit.fit(X_train, y_train)\n",
    "    #print logit.n_iter\n",
    "    #print logit.coef_\n",
    "    pred_test = logit.predict(X_test)\n",
    "    pred_train = logit.predict(X_train)\n",
    "    \n",
    "    out.write('Actual labels\\n')\n",
    "    y_test.tofile(out, sep=',')\n",
    "    out.write('\\nPredicted labels\\n')\n",
    "    pred_test.tofile(out, sep=',')\n",
    "    \n",
    "    test_score = np.mean(y_test == pred_test)\n",
    "    train_score = np.mean(y_train == pred_train)\n",
    "    out.write('\\nAccuracy: ' + str(int(100*test_score)) + '%\\n')\n",
    "    test_scores = np.r_[test_scores, test_score]\n",
    "    train_scores = np.r_[train_scores, train_score]\n",
    "    \n",
    "print 'train accuracies', train_scores.round(2)\n",
    "print 'test accuracies', test_scores.round(2)\n",
    "print 'mean training accuracy %.02f, standard deviation %.02f' %\\\n",
    "        (np.mean(train_scores), np.std(train_scores))\n",
    "print 'mean test accuracy %.02f, standard deviation %.02f' %\\\n",
    "        (np.mean(test_scores), np.std(test_scores))\n",
    "out.write('\\nmean test accuracy %.02f, standard deviation %.02f' %\\\n",
    "          (np.mean(test_scores), np.std(test_scores)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
